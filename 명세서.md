# AI 쇼핑 어시스턴트
사용 AI : 컴퓨터 비전, LLM, 추천(코사인 유사도)

컴퓨터 비전, LLM, 추천 (코사인 유사도 )을 통한 쇼핑 도우미 어플리 케이션

# 0. AI 쇼핑 어시스턴트 개요
사용자가 원하는 상품 사진을 업로드하면 객체를 인식하고, 이에 대한 정보와 유사 상품을 추천해주는 쇼핑 도우미 어플리케이션을 구현하고자합니다.

일상에서 마주치는 상품을 사진으로 찍기만 하면 원하는 상품을 찾고, AI와 대화하며 상품 정보를 얻을 수 있는 쇼핑 경험을 제공할 수 있도록 합니다.

## (1) 시나리오

- 1단계
    - 사용자 입력 : 사용자가 상품 사진 업로드
    - AI 시스템 동작 : YOLO가 상품 객체 탐지 및 분류 
    - 결과 : "빨간색 니트 가디건" 인식

- 2단계
    - 사용자 입력 : 상품 정보 요청 
    - AI 시스템 동작 : LLM이 객체 정보 기반 설명 생성
    - 결과 : 상품 특징, 스타일 설명 제공
- 3단계
    - 사용자 입력 : "가격대는 어때?" 질문
    - AI 시스템 동작 : 쇼핑 Open API + CLIP 유사도 검색
    - 결과 : 유사 상품 가격 정보 제공  
- 4단계
    - 사용자 입력 : "더 저렴한 상품" 요청 
    - AI 시스템 동작 : ChromaDB 벡터 검색 + 필터링
    - 결과 : 예산 맞춤 추천 상품 제시  
- 5단계
    - 사용자 입력 : 최종 선택         
    - AI 시스템 동작 : 구매 링크 연결            
    - 결과 : 실제 쇼핑몰 이동      

## (2) AI 기술 조합
- 컴퓨터 비전
    - AI 기능 : 객체 탐지
    - 역할 : 상품 인식 및 분류
    - 사용 모델 : YOLOv8 (DeepFashion2 데이터셋 사전학습)
- LLM
    - AI 기능 : 대화형 챗봇
    - 역할 : 고객 상담 및 안내
    - 사용 모델 : Llama 3.2 Korean Bllossom 3B

- 추천
    - 특징 추출, 유사도 기반 추천
    - 역할 : 상품 매칭 및 추천
    - 사용 모델 : CLIP VIT-B/32, ChromaDB

## (3) 그 외 기술 스택
- ChromaDB
    - 상품 특징 벡터 저장 및 유사도 검색
- LangChain 
    - LLM 파이프라인 구축
- Gradio
    - 사용자 인터페이스

---

# 1. 프로젝트 기술 스텍
어떤 기술 스텍을 사용했는지, 개념이 무엇인지 살펴보겠습니다.

## (1) ChromaDB
ChromaDB는 임베딩 벡터를 위해 설계된 오픈소스 벡터 DB로, 내부적으로 Facebook의 Faiss 라이브러리를 기반으로 HNSW(Hierarchical Navigable Small World) 알고리즘을 구현해 고차원 벡터 공간에서 최근접 이웃 검색을 수행합니다.

데이터는 컬렉션(Collection) 단위로 관리되며, 각 컬렉션은 임베딩 벡터와 함께 메타데이터를 JSON 형태로 저장할 수 있습니다. SQLite를 기반으로 로컬 환경에서도 영속성을 보장하면서 L2 거리, 코사인 유사도 등 다양한 거리 계산 지표를 지원합니다.

이 프로젝트에서는 상품 정보 (제목, 브랜드, 카테고리 등)를 KO-SRoBERTa로 임베딩하여 ChromaDB로 저장하고, 사용자 쿼리와의 벡터 유사도 계산을 통해 의미적으로 관련된 상품들을 추천하는데, 여기서 사용되는 KO-SRoBERT는 문장을 768 차원의 밀집 벡터로 인코딩하여 의미적으로 유사한 문장은 코사인 유사도가 높도록 학습된 문장 임베딩 모델입니다.

- 예시로 "검정색 원피스"를 검색하면
    - Ko-SRoBERTa: "검정색 원피스" -> [0.2, -0.5, 0.8, ...] (벡터)
    - ChromaDB : 비슷한 벡터를 가진 상품들을 찾아줌
    - 결과 : "블랙 미니 원피스", '검정색 롱 드레스"등 유사 상품 반환

## (2) LangChain
LangChain은 LLM 어플리케잇녀 개발을 위한 프레임워크입니다.
프롬프트 템플릿, 토큰 및 메모리 관리, 체인/에이전트 패턴, 벡터 DB 통합 등의 인터페이스 계층을 제공하여 복잡한 AI 워크플로우를 구조화할 수 있습니다. 이 프로젝트에서는 벡터 DB의 검색 > 문맥 구성 > LLM 추론 > 응답 생성의 파이프라인을 구축하고, 대화의 이력을 관리하는 데 사용되었습니다.

```python
# LangChain 체인 예시
# 사용자 질문 -> 벡터 DB 검색 -> 관련 상품 찾기 -> LLM에 전달 -> 응답 생성

# 실제 코드 구저
conversation_chain = ConversationalRetrievalChain(
    llm = 언어모델,
    retriever = 상품검색기,
    memory = eoghkrlfhr
)

```

## (3) CLIP(Constrastive Language-Image Pre-training)
CLIP은 OpenAI에서 개발한 멀티모달 모델로, Vision Transformer(ViT)와 텍스트 Transformer를 각각의 인코더로 사용하여 이미지와 텍스트를 동일한 512 차원 잠재 공간에 매핑합니다. 4억 개의 이미지-텍스트 쌍에 대해 매칭되는 이미지-텍스트 쌍의 코사인 유사도는 최대화하고, 비매칭 쌍의 유사도는 최소화하도록 학습되었습니다.

현재 프로젝트에서는 CLIP ViT-B/32 모델을 로드하고, 이미지의 특징 벡터를 추출하는 함수가 구현되어 있습니다.

---
# 2. 주요 기능
## (1) 패션 아이템 감지
- DeepFashion2 데이터셋이 사전 학습된 YOLOv8을 통한 패션 아이템 분류
- 총 13개 카테고리 분류 (Short sleeve top, Long sleeve top, Short sleeve outwear, Long sleeve outwear, Vest, Sling, Shorts, Trousers, Skirt, Short sleeve dress, Long sleeve dress, Vest dress, Sling dress)

## (2) 상품 검색 및 필터링
- 특정 키워드(최신, 신상, 재고, 실시간, 현재, 오늘)로 웹 검색 트리거
- 네이버 쇼핑 검색 API를 통한 상품 검색 및 데이터 정합성 처리
- 벡터 DB에서의 상품 검색


## (3) LLM 대화 처리
- 프롬프트 처리
- 토큰 및 대화 이력 관리

## (4) 웹 UI
- 객체 탐지 결과
- 대화 프롬프트
- 추천 상품 목록


---
# 3. 사전 준비
자체적인 상품 DB를 보유하고 있지 않기 때문에, 네이버 쇼핑 API를 활용하여 프로젝트를 진행하였습니다.
![alt text](image.png)